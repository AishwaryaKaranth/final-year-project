## ABSTRACT
There are millions of huge collections of stars, gas, dust and stellar remnants all held together by gravity in our vast universe. These collections, or galaxies, help in deciphering the structure and history of the universe in general. The classification of these galaxies based on morphological parameters is a relevant requirement in understanding their formation and evolution. Manual identification of the categories to which each belongs to can be tiresome, time consuming and error prone. The objective of our work was to automate the process of finding the features that characterise a galaxy using convolutional neural networks, a cardinal concept in the image data space, whilst comparing the accuracy of the classification with and without prior processing of the image dataset. The Galaxy Zoo dataset was used for the same and it was preprocessed by applying median filtering and contrast limited adaptive histogram equalisation. The final classification model was a CNN based on the VGG-16 architecture with some modifications. We considered all 37 features as per the decision tree by Willet et. al. and with a multi-regression approach, obtained a model with a validation loss of 0.0102 (mean square error) on processed images as the best performing model. The model was then deployed onto a client-side interface using Flask to predict the features of the galaxies in real-time.

## INTRODUCTION
Galaxies are the fundamental elements of the universe and the understanding of their origin and evolution is an integral part of physical cosmology. There are an uncountable number of galaxies in the cosmos, with billions not even discovered yet. The structural or morphological properties of galaxies hold the key to the deeper understanding of the process of galactic evolution. The study of the same comprises a rich history in the field of astrophysics, while answering many questions about the universe as we know it. 
The databases are generated by telescopes at an ever-increasing speed which reinforces the need for computer analysis of the images. The expanding image databases of astronomical objects such as galaxies render the process of manual methods for classification of the same as almost impossible. Experts would require enormous amounts of time to manually categorise the images of the galaxies or find the finer physical features that describe a galaxy. This led to a dire need for the creation of automated means to do the same. 
Some of the most famous datasets used by related works include the Galaxy Zoo dataset, and the Sloan Digital Sky Survey (SDSS) which is possibly the largest astronomical survey. Other data sources include - Zsolt Frei’s catalogue [3], EFIGI catalogue [8], the NGC catalogue [16], etc. Some of them went with a custom dataset constituting a random collection of images from Google [1] and other sources. After the collection of the images, the next step is to rid the images of any unwanted and hindering distortions that exist. Image preprocessing is a vital step in image enhancement which aids in the deduction of important features and inferences. Another important step is data augmentation which is performed in order to introduce more variety into the training data for the reduction of overfitting. Once the data is well prepared comes the actual process of classification. This process involves the fields of Machine Learning and Deep Learning, the flag-bearers of automating methods that require human intervention. The related works have used multiple algorithms of ML and DL. The ML methods previously tested include Support Vector Machines, Random Forest classifier, Naive Bayes classifier, K-Nearest Neighbours and Adaboost classifier [2],[3]. The DL algorithms in the related works are Artificial Neural Networks [15], and mainly different architectures of Convolutional Neural Networks such as Inception Module [1], Resnet [9], etc. Performance wise, DL fared better than ML. Some works worked on a three-category classification (Elliptical, Spiral, Irregular) [1] and some others worked on a five-category classification [2],[9]. A few works have used the Willett et.al. decision tree [21] for attempts in discovering the finer features that describe a galaxy, but some of them have cut the tree short so as to not consider all the possible features. This decision tree was also used as a reference in the famous Galaxy Zoo challenge on Kaggle which over 326 teams participating in the classification of galaxies. 
Our work on galaxy morphology classification is described in the following sections, which include the dataset under consideration, the preprocessing techniques involved, the models trained and its performance. The objective of this work is to achieve a model with a low loss using Convolutional Neural Networks, a cardinal concept in image classification, to predict the probabilities with which features characterise a given galaxy, with a multi-regression approach to the problem statement.  

## DATASET
The Sloan Digital Sky Survey has mapped nearly one-third of the sky which lead to the Oxford University possessing a dataset of millions of images of galaxies that needed to be classified based on their morphologies to better understand the galactic processes. Classifying images at this scale would indeed require a significant amount of time. Galaxy Zoo was incorporated for this very reason. Galaxy Zoo contains millions of images of galaxy and is open to the general public to classify the images of the galaxies. 
The galaxy zoo training dataset consists of 61,578 JPEG images of galaxies of size 424 * 424 with RGB channels, where each image has an ID number and galaxy at the center. The training images of this challenge were given in a single directory and their respective classes were mapped in a .csv file  with each image corresponding to 37 outputs. The test dataset in this challenge consists of 79,975 images of the galaxies without any probability labels. Therefore the task is to predict all the probabilities for the labels for the test dataset, and the result is evaluated by calculating the loss function over all predictions.

## Pre-processing

### Resizing
Images were resized to 224x224 as the architecture was designed to accept images of the same size.Resizing also proves to be a vital step as it reduces the amount of computation otherwise involved.We used the PILLOW library of python to resize the images.
### Median Filtering
The fitering technique was used to reduce noise in the images.It was chosen since it results in negligible loss since it results in negligible loss of edges.The filter has a slight smoothing effect on our images.  We employed the PIL library where ImageFilter module was used which has certain predefined filters that is used with the filter() function with the kernel size set to 3.

## MODEL ARCHITECTURE
Our work made use of a Convolutional Neural Network for the classification process. CNNs are a class of Deep Learning algorithms that constitute convolution layers (where filters are made to scan the input to the respective layer) which play a vital role in the process of feature extraction. There are multiple architectures of CNNs used in related works. Our model was inspired by the VGG-16 architecture to which we made slight modifications. The VGG-16 architecture performed well in the ImageNet challenge, and it possesses a good depth of the network while retaining the structure’s simplicity. 
We made some slight modifications to the VGG-16 architecture. The network has 16 layers in total - 13 convolution layers and 3 fully connected layers. The original VGG-16 architecture makes use of a stride of 1 in the convolutional layer filters and a max-pooling layer with a stride of 2 which halves the dimensions. We, however, decided to go with a stride of 3 for the convolutional layer filters and a stride of 1 for the max-pooling layer while retaining the sizes to be 3*3 and 2*2 respectively for the two layers, the reasons being - a larger stride length leads to more generalisation and lesser overfitting which seems to be a major problem in the case of CNNs and image classification. We also chose a stride of 1 for the max-pooling layer to not reduce the dimensions (as that is already being done in the convolutional layers due to an increased stride) and highlight the strongest features only.  The general approach is to choose a stride of 1 for the convolutional layer filters, which we also tried but we ended up getting a negligible change in the performance of the model in exchange for a much larger computational requirements. The general architecture however, remained similar to the original VGG-16. The original architecture has around 138 million parameters, while our model had around 33 million trainable parameters, with almost the same level of performance, and was much faster to train. We used ReLu (Rectified Linear Unit) as our activation function in the convolution layers and a Sigmoid activation function in the output layer which contains 37 units in accordance to the 37 features as per the decision tree considered [21].

## DECISION TREE
The decision tree considered presents the guidelines for measuring finer morphological features [21]. It stems from a citizen science crowdsourcing project where thousands of volunteers were asked to manually classify the galaxies based on 11 questions. There are 37 nodes or a total of 37 finer features that could characterise the galaxy. The output of the CNN’s final layer gives the probabilities of all the 37 features. The task at hand was to identify the answers to all the 11 questions by determining the features having maximum probability under each class (question). Once the strongest features are determined, they are presented as the physical features that characterise the input galaxy image that is uploaded by the user on the client-side interface. Each question’s answer (the one with the maximum probability), determines the flow the tree. So the decision tree was coded as a function to which the data frame containing all the 37 answers’ probabilities was given as the input. The output of the function was a list of morphological features that characterise the galaxy as per the decision tree. 

<hr>


## Model Deployment
Upon training the model,we deployed the model using flask wherein the user can upload an image and upon processsing the image the results are rendered to the user. Flask was mainly used at the serverside and at the front end we used html,css and javascript.
2 different html files are served to the browser one for uploading the image and the other for rendering the result to the user.At the backend we feed the image to the model and obtain the predicted features and render it to frontend.Http request-response protocol is implemented to communicate between the client(browser) and the server(flask).We created api endpoints to obtain user requests ,based on the request obtained from the user, we sent appropriate responses.


### Html
Html forms the basic structure of any webpage.Css was used to style our webpages.Javascript is essential to make the web page interactive and dynamic.
We used **index.html** to fetch the data from the user and included a form submit action, where on hitting a button, the data is sent to the server.The results of the prediction are served using the **pred.html**.

### Flask
Flask is a micro web framework or the server side framework written in Python.A Web application framework is the collection of libraries and modules thatenables a web application developer to write applications without having to botherabout low-level details such as protocols, thread management etc. We have used the jinja2 templating.Jinja2 templating is used to create templates and allows dynamic injection of content into the templated file.

##### Implementation steps

* Different endpoints were defined using the **route()** method that binds the url to a specific function.Upon running the flask app **GET** request is obtained at the root url, defined as **route('/')** and a corresponding base function is triggered thereby serving the browser ,the **index.html** file using the **render_template()** function.This function allows to inject the data directly into the html template file.
* The request(POST request) for prediction involves a post method and is sent to the **'/upload'** endpoint.
* Upon obtaining the image, we preprocess it [using **preprocessing()** function defined earlier] by resizing as per our model specification (224x224),and subject it to median filtering then contrast limited adaptive histogram equilization. We load the model with the weights obtained from the training process and feed the preprocessed images to it by passing it through **prediction()**.
* The output in form of a numpy array is passed to a function **get_features()** function that was previously defined(mimicked the decision tree catalog as defined in Galaxy Zoo challenge).Here while executing the function, for every corresponding responses we appended the features to a list.
* Now in order to serve the image to the user along with the features we have to store the file in a static container, instead we converted the image in the form of a data uri , a commonly used technique to directly embed images in html, this required us to temporarily save the image in a buffer using **io.BYTESIO()** class. We base64 encoded the image then converted the resulting byte to a string using the **decode()** function. This string/data usi along with the features list is injected into the **pred.html** file using **render_template()**.
* The **'pred.html'** has placeholders that are replaced by the data sent from the render function.The feature list replace the placeholder by using a for loop.
* In the front end the we have also included information about the various step taking place at the backend and rendered that using a delay.This was mainly done using javascript using **setTimeout()** function where the data is initially hidden and the visibility changes after multiple delays as per the value specified as a parameter of the setTimeout() function.

<hr/>

## CONCLUSION AND FUTURE SCOPE
The deeper understanding of the dynamical history of galaxies is aided by the powerful probe of morphological parameters. Visual inspection for the identification of characteristic physical features of galaxies is impractical for astrophysicists. Our work presents a method of automating  the process of determining the morphological features that characterise a galaxy with the use of Convolutional Neural Networks. The Galaxy Zoo dataset was resized and augmented, followed by the application of median filtering and contrast limited adaptive histogram equalisation. The processed images were then fed into a CNN based on the VGG-16 architecture. After the training of 10 models, the final model with the best combination of hyper parameters gave a mean square error of 0.0102 on processed images. The morphological features were then determined by the features having the maximum probabilities under each class or question. The model was then deployed onto a client-side interface using Flask where the user could upload an image of a galaxy and get a list of morphological features that characterise the galaxy as the output in real-time.   
There are many secondary objects present in some images, so the scale can be reduced to see if it helps focus on the features of the galaxy under consideration, or segmentation algorithms for
galaxies lacking distinct boundaries can be used. Many more data augmentation methods can be used. Techniques like flipping, translation, brightness/contrast changing etc. can be performed. Other architectures of CNNs like ResNet, AlexNet etc can be used to see if they perform any better. 
We have approached this problem as a multi-regression problem, but the probabilities of the features are not independent.  Each question’s answers apart from the first one, depends on a previous question’s answer. We believe that utilising this constraint will lead to better results.


